{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD_vnyLXZQzD"
      },
      "source": [
        "!pip install -q transformers\n",
        "# !pip install -q pytorch-lightning\n",
        "\n",
        "# Code for TPU packages install\n",
        "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvPxXdKJguYB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5c72ee92-a15e-4288-b787-beaf2f9ff679"
      },
      "source": [
        "# Checking out the GPU we have access to\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 31 11:07:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    69W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzM1_ykHaFur"
      },
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxxwd1scQNv"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2AUPG5du-L"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/news_summary.csv',encoding='latin-1')\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBoPE2bUd_Fx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1e6d4046-db95-4b57-db53-ae073e099d57"
      },
      "source": [
        "df = df[['text','ctext']]\n",
        "df.ctext = 'summarize: ' + df.ctext\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>ctext</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Administration of Union Territory Daman an...</td>\n",
              "      <td>summarize: The Daman and Diu administration on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
              "      <td>summarize: From her special numbers to TV?appe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
              "      <td>summarize: The Indira Gandhi Institute of Medi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
              "      <td>summarize: Lashkar-e-Taiba's Kashmir commander...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
              "      <td>summarize: Hotels in Mumbai and other Indian c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              ctext\n",
              "0  The Administration of Union Territory Daman an...  summarize: The Daman and Diu administration on...\n",
              "1  Malaika Arora slammed an Instagram user who tr...  summarize: From her special numbers to TV?appe...\n",
              "2  The Indira Gandhi Institute of Medical Science...  summarize: The Indira Gandhi Institute of Medi...\n",
              "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  summarize: Lashkar-e-Taiba's Kashmir commander...\n",
              "4  Hotels in Maharashtra will train their staff t...  summarize: Hotels in Mumbai and other Indian c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1Jw1Q4hqEi"
      },
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "MAX_LEN = 512\n",
        "SUMMARY_LEN = 150\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932p8NhxeNw4"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70YVNa-YiSHa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "74dfd585-a886-46ed-d4bc-411fdd309fc9"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=42).reset_index(drop=True)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ-Spz29idNS"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jKmk2eDINe"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wbgnNFCms1O"
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPAR7TWmxoM"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFAfLq7WoPqi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "bae2c5be-94bd-437c-f0b4-35f01764fc5f"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  7.654313564300537\n",
            "Epoch: 0, Loss:  1.0062143802642822\n",
            "Epoch: 0, Loss:  1.8140990734100342\n",
            "Epoch: 0, Loss:  1.5853947401046753\n",
            "Epoch: 1, Loss:  1.0318087339401245\n",
            "Epoch: 1, Loss:  1.0657906532287598\n",
            "Epoch: 1, Loss:  1.2831671237945557\n",
            "Epoch: 1, Loss:  1.473008632659912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN3yYXIz4bRf"
      },
      "source": [
        "def writer(predictions, actuals):\n",
        "    # output_path = r'/content/drive/My Drive/Colab Notebooks/'\n",
        "    # pred_file = output_path+'preds.txt'\n",
        "    # actual_file = output_path+'actuals.txt'\n",
        "    # with open(pred_file, 'w') as f:\n",
        "    #     for pred in predictions:\n",
        "    #         f.write('%s\\n' % pred)\n",
        "\n",
        "    # with open(actual_file, 'w') as f:\n",
        "    #     for actual in actuals:\n",
        "    #         f.write('%s\\n' % actual)\n",
        "    my_dict = {\n",
        "        'Actual Headline': actuals,\n",
        "        'Prediction': predictions       \n",
        "        }\n",
        "    final_output = pd.DataFrame(my_dict)\n",
        "\n",
        "    return final_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TNdHlQ0CLz"
      },
      "source": [
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPKg4Wro1Qxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b1a952f6-0d81-478d-f290-61c7f185a1ce"
      },
      "source": [
        "for epoch in range(1):\n",
        "    predictions, actuals = validate(epoch)\n",
        "    final_df = writer(predictions, actuals)\n",
        "    final_df.to_csv('/content/drive/My Drive/Colab Notebooks/predictions.csv')\n",
        "    print('Output Files generated for review')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Output Files generated for review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0-QGqoAZCxf"
      },
      "source": [
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "# input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, lm_labels=input_ids)\n",
        "# loss, prediction_scores = outputs[:2]\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "# input_ids = tokenizer.encode_plus(\"summarize: Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "# outputs = model.generate(input_ids['input_ids'])\n",
        "# pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHWrt0xZLhu"
      },
      "source": [
        "# print(f'The loss is: {loss}')\n",
        "# print(f'The Prediction score is {prediction_scores}')\n",
        "# print(f'The output is {outputs}')\n",
        "# print(f'The final output is {pred}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}